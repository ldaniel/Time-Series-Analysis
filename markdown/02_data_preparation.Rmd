---
title: "Data preparation"
date: "August, 2019"
---

```{r setup_preparation, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries
library(rmarkdown)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(ggalluvial)
library(tidyr)
library(lubridate)
library(stringr)
library(VIM)
library(psych)
library(feather)
library(tinytex)
library(knitr)
library(leaflet)
library(geojsonio)
```

```{r scripts_data_preparation, include=FALSE}
# loading required steps before performing the analysis
source("./src/util/auxiliary_functions.R")
source("./src/datapreparation/step_02_data_ingestion.R")
source("./src/datapreparation/step_03_data_cleaning.R")
```

# Using a step by step approach

Before starting the analysis for the {{__myproject__}}, a few important steps were taken in order to prepare the source data files. These steps are listed below:

- **Step 01**: Create Functions;
- **Step 02**: Data Ingestion;
- **Step 03**: Data Cleaning;
- **Step 04**: Label Translation;
- **Step 05**: Data Enhancement;
- **Step 06**: Dataset Preparation.

*******************************************************************************

# Create Functions (step 1)
This step create functions to be used in the next steps. Following, all functions created are described.

## Specific data ingestion functions

### Function 1
Description of function 1.

``` {r function_1_name, eval = FALSE}
# Function 1 code
```

### Function 2
Description of function 2.

``` {r function_2_name, eval = FALSE}
# Function 2 code
```

### Function 3
Description of function 3.

``` {r function_3_name, eval = FALSE}
# Function 3 code
```

### Function n
Description of function n.

``` {r function_n_name, eval = FALSE}
# Function n code
```

## Metrics auxiliary functions

### calculateModelMetrics
The objective of this function is to calculate main metrics of model performance according to a cutoff value.

``` {r calculateModelMetrics, eval = FALSE}
calculateModelMetrics <- function(cutData, realData, predData){
  cuttedData <- as.factor(ifelse(predData>=cutData, 1, 0))
  
  invisible(capture.output(out <- CrossTable(realData, cuttedData, 
                                             prop.c = F, prop.t = F, prop.r = T, prop.chisq = F)))
  
  out <- as.data.frame(out) %>% 
    mutate(merged=paste0(t.x, t.y)) %>% 
    dplyr::select(merged, val=t.Freq)
  
  TN <- filter(out, merged == "00")$val[1]
  FP <- filter(out, merged == "01")$val[1]
  FN <- filter(out, merged == "10")$val[1]
  TP <- filter(out, merged == "11")$val[1]
  
  return(data.frame(Cut = cutData,
                    TN = TN, 
                    FP = FP,
                    FN = FN, 
                    TP = TP,
                    TPR = TP/(TP+FN), TNR=TN/(TN+FP),
                    Error = (FP+FN)/(TP+TN+FP+FN),
                    Precision = TP/(TP+FP),
                    F1 = 2*(TP/(TP+FN))*(TP/(TP+FP))/((TP/(TP+FP)) + (TP/(TP+FN)))))
}
```

### modelMetrics  
The objective of this function is to calculate main metrics of model performance for cutoffs from 0-1 based on given step.

``` {r modelMetrics, eval = FALSE}
modelMetrics <- function(realData, predData, stepping = 0.01, 
                         plot_title = "TPR/TNR by cutoff over full dataset"){
  probCuts <- seq(from = 0, to = 1, by = stepping)
  out <- bind_rows(lapply(probCuts, calculateModelMetrics, realData = realData, predData = predData))
  out <- out[complete.cases(out),] %>% mutate(Difference = abs(TPR-TNR))
  
  best <- out %>% arrange(Difference) %>% head(1) %>% dplyr::select(-Difference)
  
  p <- plot_ly(x = ~out$Cut, y = ~out$Difference, name = 'Abs. Diff.', type = 'bar', opacity = 0.3) %>% 
    add_trace(x = ~out$Cut, y = ~out$TPR, name = 'TPR', type = 'scatter', mode = 'lines', opacity = 1) %>% 
    add_trace(x = ~out$Cut, y = ~out$TNR, name = 'TNR', type = 'scatter', mode = 'lines', opacity = 1) %>% 
    layout(xaxis = list(title = "Cutoff Value"),
           yaxis = list(title = "True Ratio (%)")) %>%
    add_annotations(
      text = sprintf("<b>%s</b>", plot_title),
      x = 0,
      y = 1.04,
      yref = "paper",
      xref = "paper",
      xanchor = "left",
      yanchor = "top",
      showarrow = FALSE,
      font = list(size = 15)
    ) %>%
    add_annotations(
      text = sprintf("<b>%s</b>", best$Cut),
      x = best$Cut,
      y = best$TPR,
      showarrow = FALSE,
      bgcolor = "white",
      opacity = 0.8
    )
  
  return(list(TableResults = out,
              BestCut = best,
              Plot = p))
}
```

## Data preparation functions

#### createTestAndTrainSamples
See topic "Splitting dataset into Train and Test data" for further details.

## Plot auxiliary functions

Functions used in the evaluation step to compare the models.

### Score_Histograms

Function used to plot the score density plots of the model.

Needs to receive a dataset containing the predicted and actual values, the actual values vector the score (predicted) values value and a custom title.

``` {r Score_Histograms, eval = FALSE}
Score_Histograms <- function(dataset, predicted, actual, title) {
  ggplot(data = dataset) +
    geom_density(aes(x = predicted, fill = as.factor(actual)),
                 alpha = 0.5) +
    scale_fill_manual(values = c("0" = "#16a085", "1" = "#e74c3c")) +
    scale_x_continuous(limits = c(0, 1)) +
    theme_economist() +
    labs(title = title,
         y = 'Score',
         fill = 'Defaulter |1 = True|') +
    theme(panel.grid = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          legend.position = 0,
          plot.title = element_text(hjust = 0.5))
}
```

### Score_Boxplot

Function used to plot the score box plot of the model.

Needs to receive a dataset containing the predicted and actual values, the actual values vector the score (predicted) values value and a custom title.

``` {r Score_Boxplot, eval = FALSE}
Score_Boxplot <- function(dataset, predicted, actual, title) {
  ggplot(data = dataset) +
    geom_boxplot(aes(y = predicted,
                     fill = as.factor(actual))) +
    coord_flip() +
    scale_fill_manual(values = c("0" = "#16a085", "1" = "#e74c3c")) +
    scale_y_continuous(limits = c(0, 1)) +
    theme_economist() +
    labs(title = title,
         y = 'Score',
         fill = 'Defaulter |1 = True|') +
    theme(panel.grid = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          legend.position = 0,
          plot.title = element_text(hjust = 0.5))
}
```

### KS_Plot

Function used to plot the cumulative probability distribution and KS metric of the model.

Needs to receive a vector with scores of Defaulters and a vector f scores of Non-Defaulters and a custom title.

``` {r KS_Plot, eval = FALSE}
KS_Plot <- function(zeros, ones, title) {
  group <- c(rep("Non Defaulters", length(zeros)), rep("Defauters", length(ones)))
  dat <- data.frame(KSD = c(zeros, ones), group = group)
  cdf1 <- ecdf(zeros) 
  cdf2 <- ecdf(ones) 
  minMax <- seq(min(zeros, ones), max(zeros, ones), length.out=length(zeros)) 
  x0 <- minMax[which( abs(cdf1(minMax) - cdf2(minMax)) == 
                        max(abs(cdf1(minMax) - cdf2(minMax))) )][1] 
  y0 <- cdf1(x0)[1]
  y1 <- cdf2(x0)[1]
  ks <- round(y0 - y1, 2)
  
  ggplot(dat, aes(x = KSD, group = group, color = group))+
    stat_ecdf(size=1) +
    geom_segment(aes(x = x0[1], y = y0[1], xend = x0[1], yend = y1[1]),
                 linetype = "dashed", color = "blue") +
    geom_point(aes(x = x0[1] , y = y0[1]), color="blue", size=4) +
    geom_point(aes(x = x0[1] , y = y1[1]), color="blue", size=4) +
    geom_label(aes(x = x0[1], y = y1[1] + (y0[1] - y1[1]) / 2, label = ks),
               color = 'black') +
    scale_x_continuous(limits = c(0, 1)) +
    labs(title = title,
         y = 'Cumulative Probability Distribution',
         x = 'Score') +
    theme_economist() +
    theme(legend.title = element_blank(),
          panel.grid = element_blank(),
          legend.position = 0,
          plot.title = element_text(hjust = 0.5))
}
```

### Plot_ROC

Function used to plot the combined ROC curves of each model.

Needs to receive a dataset with actual and predicted scores of each model.

``` {r Plot_ROC, eval = FALSE}
Plot_ROC <- function(dataset, smooth_opt = FALSE) {
  roc_logistic      <- roc(logistic.actual ~ logistic.predicted,
                           dataset,
                           smooth = smooth_opt,
                           quiet = TRUE)
  
  roc_decision.tree <- roc(decision.tree.actual ~ decision.tree.predicted,
                           dataset,
                           smooth = smooth_opt,
                           quiet = TRUE)
  
  roc_boosting      <- roc(boosting.actual ~ boosting.predicted,
                           dataset,
                           smooth = smooth_opt,
                           quiet = TRUE)
  
  roc_random.forest <- roc(random.forest.actual ~ random.forest.predicted,
                           dataset,
                           smooth = smooth_opt,
                           quiet = TRUE)
  
  p <- ggplot() +
    geom_line(aes(x = 1 - roc_logistic$specificities, 
                  y = roc_logistic$sensitivities, 
                  colour = 'Logistic Regression'), # red
              size = 1,
              linetype = 1,
              alpha = 0.7) +
    geom_line(aes(x = 1 - roc_decision.tree$specificities, 
                  y = roc_decision.tree$sensitivities,
                  colour = 'Decision Tree'), # blue
              size = 1,
              linetype = 1,
              alpha = 0.7) +
    geom_line(aes(x = 1 - roc_boosting$specificities, 
                  y = roc_boosting$sensitivities,
                  colour = 'Boosting'), # green
              size = 1,
              linetype = 1,
              alpha = 0.7) +
    geom_line(aes(x = 1 - roc_random.forest$specificities, 
                  y = roc_random.forest$sensitivities,
                  colour = 'Random Forest'), # purple
              size = 2,
              linetype = 1,
              alpha = 1) +
    geom_abline(aes(intercept = 0, slope = 1),
                linetype = 2,
                size = 1) +
    scale_colour_manual(name = NULL,
                        breaks = c('Logistic Regression', 
                                   'Decision Tree',
                                   'Boosting', 
                                   'Random Forest'),
                        labels = c('Logistic Regression', 
                                   'Decision Tree',
                                   'Boosting', 
                                   'Random Forest'),
                        values = c('#C0392B', 
                                   '#3498DB', 
                                   '#28B463', 
                                   '#9B59B6')) +
    labs(y = 'True Positive Rate',
         x = 'False Positive Rate',
         title = 'Receiver Oerating Characteristic Curve - ROC',
         subtitle = 'Random Forest and Boosting are the models that best discriminate Defaulters and Non-Defaulters') +
    theme_economist() +
    theme(panel.grid = element_blank())
  
  return (p)
}
```

### accuracy

Function used to output confusion matrix and basic accuracy metrics of each model.

Needs to receive a vector of actual and predicted values.

``` {r accuracy, eval = FALSE}
accuracy <- function(score, actual, threshold = 0.5) {
  
  fitted.results <- ifelse(score > threshold ,1 ,0)
  
  misClasificError <- mean(fitted.results != actual)
  
  misClassCount <- misclassCounts(fitted.results, actual)
  
  print(kable(misClassCount$conf.matrix))
  
  print('--------------------------------------------------------------')
  print(paste('Model General Accuracy of: ', 
              round((1 - misClassCount$metrics['ER']) * 100, 2), '%', 
              sep = ''))
  print(paste('True Positive Rate of    : ', 
              round(misClassCount$metrics['TPR'] * 100, 2), '%',
              sep = ''))
}
```

*******************************************************************************

# Data Ingestion (step 2)
The process of data ingestion — preparing data for analysis — usually includes steps called extract (taking the data from its current location), transform (cleansing and normalizing the data), and load (placing the data in a database where it can be analyzed).

During this step, in addition to the loading data processes, it was performed data casting, column renaming and small touch-ups. The list below describe each table adjustment taken:

- **Table 1**: short description about what was done in the table 1;
- **Table 2**: short description about what was done in the table 2;
- **Table 3**: short description about what was done in the table 3;
- **Table n**: short description about what was done in the table n;

*******************************************************************************

# Data Cleaning (step 3)

The objective of this step is analysing missing values and other strange conditions. In order to accomplish this task, a few R functions were used to quickly discover missing values, like NA and empty fields.

First thing done, was fixing observations in k_symbol transaction table with ' ' (one space) to empty string (''), using the following command.

Then, the command below was used to find out any NA values in each table.

``` {r find_na, eval = FALSE}
sapply(TableName, function(x) sum(is.na(x)))
```

Solely the **transaction** table has NA values, in the following columns:

```{r transaction_na_cols, echo=FALSE, results = 'asis'}
kable(mytable_na_cols)
```

Finally, the following command was used in each table to find out where empty values was hidden. 

``` {r find_empty, eval = FALSE}
sapply(TableName, function(x) table(as.character(x) =="")["TRUE"])
```

Again, only the **transaction** table had empty values, according to the table below:

```{r echo=FALSE, results = 'asis'}
kable(mytable_empty_cols)
```

For the exploration analysis report, we did not take any additional action, since the missing values was not relevant.

*******************************************************************************

# Label Translation (step 4)
In order to make the data information more understandable, it was translated some relevant labels and domains from Czech to English.

``` {r translate, eval = FALSE}
# Translating relevant labels and domains to english --------------------------------------------

mytable$x_var_1 <- plyr::mapvalues(mytable$x_var_1, 
                                   c('Other_Language_Column_Name1', 
                                     'Other_Language_Column_Name2', 
                                     'Other_Language_Column_Name3'),
                                   c('English_Column_Name1', 
                                     'English_Column_Name2', 
                                     'English_Column_Name3'))

```

*******************************************************************************

# Data Enhancement (step 5)
This step aims to improve the analysis by adding auxiliary information. Data enhancement is all about making sure any data that is coming into the business is being looked at with a critical eye and is being filtered down to maximize its value.

<Describe what kind of data enhancement you have to do. Include chuncks of code to ilustrate what is necessary.>

``` {r client, eval = FALSE}
# your code with the enhancements goes here
```

*******************************************************************************

# Data Preparation for Predictive Modeling (step 6)

## Selecting the target dataset

The below function was created to be used in the modeling exercises to be performed, the idea is to have a standard way to get the prepared data set already prepared with correct data types and dummies.

``` {r data_prep, eval = FALSE}
# your code with the data preparation goes here
```

## Splitting dataset into Train and Test data
The below function was created to be used in the modeling exercises to be split the source_dataset into train and test datasets.

``` {r split_func, eval = FALSE}
mydataset <- fgvr::createTestAndTrainSamples(dataset = you_dataset_name, yvar = "your_y_var", 
                                             seed = 12345, percentage = 0.7)
```

To make sure all the models uses the same datasets for Train and Testing we are saving the initial sampling to be reused across the models.

This will ensure consistency when comparing the models against each other.

``` {r save_dataset, eval = FALSE}
# calling function to split and create train and test databases
# this function will split the dataset into train and test data and save the sampling in disk
# to resample just delete './models/source_train_test_dataset.rds' file and rerun this script
if (file.exists('./models/source_train_test_dataset.rds')) {
  source_train_test_dataset <- readRDS('./models/source_train_test_dataset.rds')
} else {
  source_train_test_dataset <- mydataset
  saveRDS(source_train_test_dataset, './models/source_train_test_dataset.rds')  
}
```