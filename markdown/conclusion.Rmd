---
title: "Conclusion"
date: "August, 2019"
---

```{r setup_evaluation, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries and scripts
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggcorrplot)
library(ggpubr)
library(plotly)

# libraries for data clean
library(VIM)
library(rms)
library(mctest)

# libraries for modeling
library(caret)
library(gmodels)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)

# libraries for measures
library(hmeasure)
library(pROC)
```

```{r scripts_evaluation, include=FALSE}
# loading required steps before performing the analysis
source("./src/util/auxiliary_functions.R")
source("./src/datapreparation/step_02_data_ingestion.R")
source("./src/datapreparation/step_03_data_cleaning.R")
source("./src/datapreparation/step_04_label_translation.R")
source("./src/datapreparation/step_05_data_enhancement.R")
source("./src/datapreparation/step_06_dataset_preparation.R")
```

# Objective

Describe the objetive of this report.

*******************************************************************************

# Model evaluation

## Getting the predicted score from each model

We will start this task by consolidating all the actual and predicted score for all models in a single data frame for the full, train and test datasets.

```{r get_scores_evaluation, echo=TRUE}
# your code goes here
```

## Getting performance measures for each model.

To calculate the performance measures, derived from the confusion matrix, of each model we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

We will use the custom functions described in Auxiliary metrics functions topic in the Data Preparation session of this site.

With the returned object from these functions we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures_evaluation, echo=TRUE, out.width= '100%'}
## getting cut off measures -----------------------------------------------------------------
# your code goes here
```

Having the best score cut off for each model we use **HMeasure()** function from **hmeasure** library to calculate the full set of metrics for classification methods.

```{r get_measures_full_evaluation, echo=TRUE, out.width= '100%', warning=FALSE}
# your code goes here
```

Below are the metrics on the train dataset:
```{r see_train_measures_evaluation, echo=TRUE, out.width= '100%'}
# your code goes here
```

Below are the metrics on the test dataset:
```{r see_score_measures_evaluation, echo=TRUE, out.width= '100%'}
# your code goes here
```

## Evaluating performance of each model

In this session we interpret the set of metrics we got from above steps.

### Density Plots

Here we look back to the score density plot produced by each model side by side.

Interesting enough to notice here how narrow the scores produced from Decision Tree model compared to the other models.

As discussed in the Decision Tree session this model is extremely limited for this dataset with a huge variance in the tree depending on the train data and hyper parameters tuning.

We are using our custom function **Score_Histograms** described in the Function topic of Data Preparation session of this site.

```{r density_plots_evaluation, echo=TRUE, out.width= '100%'}
# your code goes here
```

### Score Boxplots

Let's now look at the score boxplots of each model side by side.

This plot is a great way to visualize how well each model discriminate Defaulters and Non-Defaulters.

Here we can see that Random Forest and Boosting have an edge on the classification power against the other models. Both of them have not presented interquartile overlap in their box plot, another important sign of discrimination power of the models.

Our Decision Tree is right on the limit.

```{r boxplots_plots_evaluation, echo=TRUE, out.width= '100%'}
# your code goes here
```

### KS Plots

Now we look to the custom KS plots of each model side by side.

The KS metric is the maximum distance between the cumulative distribution functions of two samples.

KS metric ranges from 0 to 1, 0 meaning that there is no discrimination at all and 1 meaning a full discrimination.

In this case we are comparing the cumulative distribution sample of Defaulters and Non-Defaulters.

The bigger the KS metric the better the model are to discriminate Defaulters to Non-Defaulters.

```{r KS_plots_evaluation, echo=TRUE, out.width= '100%'}
# your code goes here
```

### ROC Curve

Now we look to the ROC curve of our models.

The Receiver Operating Characteristic Curve is a plot that shows the discrimination ability of a binary classifier model as its classification threshold changes.

We get this chart by plotting the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at different threshold settings.

The bigger the Area Under the Curve (AUC) the better the model is in classifying the observation.

```{r ROC_evaluation, echo=TRUE, out.width= '100%', warning=FALSE}
# your code goes here
```

### Acurracy

We finally look at the Confusion Matrix at the best cut off of each model to get a sense of the accuracy we were able to get in this exercise.

We will use the custom functions described in Auxiliary metrics functions topic in the Data Preparation session of this site.

Boosting is the best model we got in this exercise following very closely by Random Forest.

```{r accuracy_evaluation, echo=TRUE, out.width= '100%'}
# your code goes here
```

# Final considerations and project limitations

Describe your final considerations and project limitations here.